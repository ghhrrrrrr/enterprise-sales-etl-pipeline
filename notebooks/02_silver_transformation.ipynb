{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41271e50-a453-43d0-bda2-327f0c126f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, round, current_timestamp, to_date, lit, abs, lower, coalesce\n",
    "from pyspark.sql.types import DecimalType, LongType, IntegerType\n",
    "\n",
    "BRONZE_TABLE = \"workspace.retail.bronze_sales\"\n",
    "SILVER_TABLE = \"workspace.retail.silver_sales\"\n",
    "AUXILIARY_TABLE = \"workspace.retail.silver_auxiliary_sales\"\n",
    "QUARANTINE_TABLE = \"workspace.retail.silver_quarantine_sales\"\n",
    "\n",
    "OPS_CODES = [\"POST\", \"D\", \"M\", \"BANK CHARGES\", \"ADJUST\", \"S\",\"DOT\", \"AMAZONFEE\" , \"C2\", \"B\"]\n",
    "\n",
    "def cast_and_enrich(df):\n",
    "    df_casted = df.withColumn(\"quantity\", col(\"quantity\").cast(IntegerType())) \\\n",
    "        .withColumn(\"price\", col(\"price\").cast(DecimalType(12, 2))) \\\n",
    "        .withColumn(\"customer_id\", col(\"customer_id\").cast(LongType())) \\\n",
    "        .withColumn(\"invoice_date_only\", to_date(col(\"invoice_date\"))) \n",
    "\n",
    "    df_filled = df_casted.withColumn(\"customer_id\", coalesce(col(\"customer_id\"), lit(-1).cast(LongType())))\n",
    "\n",
    "    df_enriched = df_filled.withColumn(\"total_amount\", round(col(\"quantity\") * col(\"price\"), 2).cast(DecimalType(18, 2))) \\\n",
    "        .withColumn(\"row_category\", lit(\"BATCH\")) \\\n",
    "        .withColumn(\"meta_silver_load_at\", current_timestamp()) \n",
    "\n",
    "    business_cols = [\"invoice\", \"stock_code\", \"customer_id\", \"quantity\", \"price\", \"total_amount\", \"description\", \"country\", \"invoice_date\"]\n",
    "    tech_cols = [\"invoice_date_only\", \"row_category\", \"file_path\", \"env\"]\n",
    "    meta_cols = [c for c in df_enriched.columns if c.startswith(\"meta_\")]\n",
    "    final_column_order = business_cols + tech_cols + meta_cols\n",
    "\n",
    "    df_final = df_enriched.select(*final_column_order)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "def classify_and_split(df):\n",
    "    \n",
    "    df_classified = df.withColumn(\"row_category\",\n",
    "        when(col(\"stock_code\").isin(OPS_CODES), \"OPS_FEE\")\n",
    "        .when(lower(col(\"stock_code\")).contains(\"gift\"), \"OPS_GIFT\")\n",
    "        .when((col(\"customer_id\") == -1) & (col(\"quantity\") < 0) & (col(\"price\") == 0), \"INVENTORY_ADJUSTMENT\")\n",
    "        .when((col(\"invoice\").startswith(\"C\")) & (col(\"price\") > 0), \"RETURN\")\n",
    "        .when((col(\"quantity\") > 0) & (col(\"price\") > 0), \"SALE\")\n",
    "        .otherwise(\"INVALID\")\n",
    "    )\n",
    "    \n",
    "\n",
    "    silver_sales = df_classified.filter(col(\"row_category\").isin(\"RETURN\", \"SALE\")) \\\n",
    "        .drop(\"row_category\")\n",
    "\n",
    "    silver_auxiliary = df_classified.filter(col(\"row_category\").isin(\"OPS_FEE\", \"OPS_GIFT\", \"INVENTORY_ADJUSTMENT\"))\n",
    "    \n",
    "    silver_quarantine = df_classified.filter(col(\"row_category\") == \"INVALID\")\n",
    "\n",
    "\n",
    "    return silver_sales, silver_auxiliary, silver_quarantine\n",
    "\n",
    "def main():\n",
    "    bronze_df = spark.table(BRONZE_TABLE)\n",
    "\n",
    "    enriched_df = cast_and_enrich(bronze_df)\n",
    "    \n",
    "    silver_df, auxiliary_df, quarantine_df = classify_and_split(enriched_df)\n",
    "    \n",
    "    print(f\"Writing {silver_df.count()} rows to Silver...\")\n",
    "    silver_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(SILVER_TABLE)\n",
    "    \n",
    "    print(f\"Writing {auxiliary_df.count()} rows to Auxiliary...\")\n",
    "    auxiliary_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(AUXILIARY_TABLE)\n",
    "\n",
    "    print(f\"Writing {quarantine_df.count()} rows to Quarantine...\")\n",
    "    quarantine_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(QUARANTINE_TABLE)\n",
    "        \n",
    "    print(\"Silver Stage Completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
